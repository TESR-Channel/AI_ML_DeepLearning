{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "3.TensorFlow_keras_Detect_Number_Prediction_Part.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1xHF8cBienK2HbkLdaKGFfvlERQG749Dd",
      "authorship_tag": "ABX9TyMKi6XEY8E2FDiNtvAYqQnG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/anoney/AI_ML_DeepLearning/blob/master/3_TensorFlow_keras_Detect_Number_Prediction_Part.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jbrt-BXzaTac",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 120
        },
        "outputId": "29357cec-826b-4a14-ef70-a1accc068d23"
      },
      "source": [
        "# TensorFlow and tf.keras\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "# Helper libraries\n",
        "import numpy as np\n",
        "import cv2\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# load an image\n",
        "Testing_img_path = \"./drive/My Drive/Colab Notebooks/3.TensorflowKerasDetectNumber/Testing_image/0.jpg\"\n",
        "input_image = cv2.imread(Testing_img_path,cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "# Preprae data\n",
        "input_image = cv2.resize(input_image,(28, 28))\n",
        "input_image = cv2.bitwise_not(input_image) # invert Black to White like data input when trained (Line is White)\n",
        "input_image = np.expand_dims(input_image, axis=0) # Change the shape of image array like input image when trained\n",
        "\n",
        "# ***load model***\n",
        "loadpath = \"./drive/My Drive/Colab Notebooks/3.TensorflowKerasDetectNumber/num_reader.model\"\n",
        "model = tf.keras.models.load_model(loadpath)\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(input_image)\n",
        "print(np.argmax(predictions))\n",
        "\n",
        "# Visualize, check the answer\n",
        "Original_input_image_testing = cv2.imread(Testing_img_path)\n",
        "print(\"This is what your computer has seen.\")\n",
        "cv2_imshow(Original_input_image_testing)\n",
        "cv2.waitKey(0)\n",
        "cv2.destroyAllWindows()\n"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:10 out of the last 10 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7fb56664c8c8> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "0\n",
            "This is what your computer has seen.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAFgklEQVR4nH1W308TSxQ+Z2a621JLaVNRSBNrpA9NNfHifVAk/iCSGOL1vuir8dV/zkQMxhAiSTWpBiUhl4RGIgVJW8PSIoUKdnfm3IcDYwMtkz5sd+d859c33xkkIui2iAgRiYiIhBCnvwIAItqdpxG01lLKk5Z2sQ0iMroxxhjDn4wxiMgb7HtjjNa6E4EN0ZzKwAZjjCEiKSWA4UiJyBhQSnWmaKM5kf2Rm14ZcAhSSiLSWrMNIkopASAIAhs4ItrkbGT2jTIUdEX3fR8RQ1LZoBjX8+qFQmFpaSkcDodCIdd1Jycnc7mcddNZPSJSvcJ3HdeQIUO8VSmFiEopz/MWFxe/fPnium673U4kEtlsNplMXrx4USnFsXeSoqcDQ4aIUKAxBqUAxMDoer3+6dOn2dnZpaUlKaXW2nXdvr4+IUQ+n0+n06FQiFsFx03u6cBWUwghUBwcHszPz09PT/+3tLy2tsboUsrfv3+/ffu22Ww+f/7cdd3h4WFu258e9ILmmiKiQAEAkXCkWCzOzs5WNqvMSKWUECIUCm1vb6+srFSrVaYpo1uW9y6RMUIIItJGSyFbv1qtVsvzPN/3Of3BwcFoNKq1LpfLjuNIKZlgtslcq7MyECiI4Nu3b8VicW5u7v3799o34XD43r17ExMTQ0ND3JJarcb7ufSMa89BTwdaa6EEEZVKpenp6ZcvX7JZPB6fnJx88eJFJBIplUqNRqOvr+/w8NAa2iZzKl0cIAEASFQIQuv29vZ2s9kUQjhO2Pf9RCKRTqcjkYjv+zs7O3t7exaXH5g8No+eJ5l5AgD7+/t7e3ta64ODAyIaGBjo7+/nABuNxtbWVrvddhxHa221CI8XACjsIqYCAHzfl1IaA1tb27u7e0IoAEilUrlcbmBggIg2Nze/fv1aLpd933ccx3EcJkVnNmf1QEophPB9v1Kp1Go1REyn0xMTE48eTeXzeSJ69+7dmzdvPn78CACxWOzcuXORSITJbXUQEQUIJAQDRAjHDwbE0QzY3NxYX1/b3d0BMHfujP/77z9jY2OxWOzXr1/VanVjY4NV5PLly6N//xWJhgkMoTGgCQ3/zjrJ379/X1xcrNVqLNrXr18fHx9PJpMA8PPnz1arxTFmMplr165lMhkllTaaXf5p8glBt2fk4OBgdXV1YWGh0WgAQBAELHaI+OPHj0KhsLy8XK/XY7HY48eP79+/r+TRkBBC4PFMEXhqFtrVarUajUatVmOOx+NxKaXrugCwsrIyPz9fLpd3dnYQ8caNGyMjI37gE5CU8sQoPOugcbs4xUQiMTg4iIie5xWLxUKhUKlUACCZTMbj8f7+fgMaAQ0ZYwwIECgIKNDBWVLBvOZAmOmfP3/+8OHDzMzM+vp6NBp98ODB1NRUPp8HAIt+NG3oaPgoOJaRzvPdqSpExCq/vLxcKpVevXq1urrabrdzudzDhw+fPXsWDoeDICA03AApJHvi57PU1HEc13UR0fd9z/NmZmZc161UKoeHh/F4fHR0NJvNhsNhY4yUElAQEMfO6IiojT5qfSeR+K+U0s5YAGg2m/v7+9yYoaGhsbGxu3fvjoyMWFXnmhyR51iOztKiUCh04cKFTCbDyhMEAds4jjM+Pv7kyZNbt26dP39eCMFee+H0LFE0Gr1582YqlapWq69fv67X61euXMlms8PDw0+fPr19+7bjOHA8v6SUXJ/TCwPjd/0gQDKR5ubmFhYWPM/LZDKXLl1KpVJXr15NJpPtdlsIoZTSWgshDOiuOD0dSFRBEHB9eRby4lkPnTNVawBA2RWmtwMwVtVPXmz5omfFmff0yqBnD+xcte3l+wifEiFEp/prrXtlcNZEA3szUIq1xTKYiIIgOHHH6rr+ByqnqJbCbl6sAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<PIL.Image.Image image mode=RGB size=32x32 at 0x7FB5644BC048>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}